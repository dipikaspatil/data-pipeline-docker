# Data Pipeline in Docker
Creating a simple data pipeline in Docker involves defining a few key stages that simulate data flow â€” e.g., ingestion â†’ transformation â†’ storage â€” and running each stage as a containerized service.

An example pipeline:

Ingest data using a Python script (e.g., CSV or API).
Transform data with another Python service.
Store data into a lightweight database like PostgreSQL or just a file.

# ğŸ³ Dockerized Data Pipeline (Ingest â†’ Transform)

This is a simple data pipeline built using **Python** and **Docker Compose**. It simulates a two-step process:

1. **Ingest:** A Python script writes sample data to `raw.csv`
2. **Transform:** Another script reads that file, modifies it, and writes to `clean.csv`

---

## ğŸ“‚ Project Structure

data-pipeline-docker/
â”œâ”€â”€ docker-compose.yml
â”œâ”€â”€ Dockerfiles/
â”‚ â”œâ”€â”€ Dockerfile.ingest
â”‚ â””â”€â”€ Dockerfile.transform
â”œâ”€â”€ ingest/
â”‚ â””â”€â”€ ingest.py
â”œâ”€â”€ transform/
â”‚ â””â”€â”€ transform.py
â”œâ”€â”€ shared_data/
â””â”€â”€ README.md


---

## â–¶ï¸ How to Run

> ğŸ§° Requirements: Docker & Docker Compose installed on your system

```bash
# Build the containers
docker-compose build

# Run the pipeline
docker-compose up

After running, you'll find the output in the shared_data/ folder:

raw.csv â†’ created by the ingest step

clean.csv â†’ created by the transform step

# Clean up after run

docker-compose down

